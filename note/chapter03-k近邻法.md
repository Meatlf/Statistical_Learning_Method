# chapter03-k近邻法
**总结**:

1.我们提出了k近邻算法，算法的核心思想是，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。更通俗说一遍算法的过程，来了一个新的输入实例，我们算出该实例与每一个训练点的距离（这里的复杂度为0(n)比较大，所以引出了下文的kd树等结构），然后找到前k个，这k个哪个类别数最多，我们就判断新的输入实例就是哪类;

2.与该实例最近邻的k个实例，这个最近邻的定义是通过不同**距离函数**来定义，我们最常用的是欧式距离;

3.为了保证每个特征同等重要性，我们这里对每个特征进行**归一化**;

4.k值的选取，既不能太大，也不能太小，何值为最好，需要实验调整参数确定！



**Q**:如何评价k近邻算法?

**A**:

1)**优点**:

- 非参数的分类技术,简单直观,易于实现!**只要让预测点分别和训练数据求距离,挑选前k个即可,非常简单直观.**
- k近邻法是一种在线技术,新数据可以直接加入数据集而**不必进行重新训练**.

2)**缺点**:

- 当样本不平衡时，**比如一个类的样本容量很大，其他类的样本容量很小**，输入一个样本的时候，K个邻近值大多数都是大样本容量的那个类，这时可能会导致分类错误。

  **改进方法：对K邻近点进行加权，也就是距离近的权值大，距离远的点权值小。**

- 计算量较大，**每个待分类的样本都要计算它到全部点的距离**，根据距离排序才能求得K个临近点。

  **改进方法**：先对已知样本带你进行裁剪，事先去除分类作用不大的样本，采取kd树以及其它高级搜索方法BBF等算法减少搜索时间.

**Q**:何为k近邻法(k-nearest neighbor,k-NN)?  

**A**:  

1)k近邻法是一种**基本分类与回归方法**;  

2)本书只讨论关于k近邻的**分类问题**.

**Q**:k近邻的**输入输出**的什么?

**A**:k近邻的输入为实例的特征向量,输出为实例的类别(可以是多类别).

**Q**:如何理解k近邻?

**A**:

![img](https://pic3.zhimg.com/80/v2-c3f1d2553e7467d7da5f9cd538d2b49a_720w.png)

[1]对上图进行了详细的解释,理解了该图,就算大体理解了k近邻算法.关于k近邻算法有如下2个要点:

1)根据其**k个最近邻**的训练实例的类别,通过**多数表决**(少数服从多数)等方式进行预测;

2)由1)可知,kNN**不具有显式的学习过程**.

**Q**:k近邻的**基本要素**有哪些?

**A**:

1)**k值的选择**;

2)**距离度量**;

3)**分类决策规则**.

## 3.1 k近邻算法

**Q**:k近邻算法实现的**大致过程**是什么?  

**A**:给定一个训练数据集,对新的输入实例,在训练数据集中找到与该实例最邻近的k个实例,这k个实例的多数属于某个类,就把该输入实例分为这个类.

## 3.2 k近邻模型
**Q**:如何理解**k近邻模型**? 

**A**:k近邻法使用的模型实际上对应于**特征空间的划分**.模型由三个基本要素:距离度量,k值的选择和分类决策规则决定.

### 3.2.1 模型

**Q**:如何从直观上理解k近邻模型?

**A**:k近邻法的模型对应特征空间的一个划分,具体见图3.1.

### 3.2.2 距离度量

**Q**:为什么要研究距离度量?

**A**:特征空间中两个实例点的距离是2个实例点相似程度的反映,分类的原则也是根据相似程度而进行分类,因而有必要研究距离度量.

### 3.2.3 k指的选择

**Q**:为什么要研究k值的选择?

**A**:k值的选择对k近邻法的结果产生重大影响.

**Q**:如何k值选取的过小或过小大会怎样?

**A**:[1]用了图文结合的方式说明了k值选取过小或过大造成的影响:

1)**k值过小**:模型变得复杂,"学习"的估计误差(estimation error)会增大,发生过拟合.(其实这3小句话讲的是同一个意思.)

2)**k值过大**:模型变得简单,"学习"的近似误差(approximation error)会增大.

**Q**:在实际应用中,如何选择k?

**A**:在应用中,k值一般选择的较小,通常使用**交叉验证法**来选取最优的k值.(**也就是说，选取k值很重要的关键是实验调参，类似于神经网络选取多少层这种，通过调整超参数来得到一个较好的结果**).

**特征归一化的必要性**:

"特征归一化的必要性"是[1]补充的内容,很值得学习.博主通过身高和脚码作为基于k近邻算法的男女生分类模型的特征,说明**特征归一化**十分有必要.

### 3.2.4 分类决策规则

**小结**:本小节从概率的角度说明了多数表决规则等价于经验风险最小化.

## 3.3 k近邻法的实现:kd树
### 3.3.1 构造kd树
**Q**:如何构造kd树?  

**A**:[1]将书本中例3.2一步一步进行拆解,基于二叉树和中位树这两个概念,通过图文结合的方式来使读者理解如何构造kd树.

**说明**:对于某个集合,可能存在2个中位数,[1]和书本均使用2者中较大的数作为中位数,选择较大的还是较小的无所谓,但是在一个系统里面最好一致就行.

### 3.3.2 搜索kd树

**Q**:如何搜索kd树?

**A**:[1]参考书本中的例3.3一步一步进行拆解,通过图文结合的方式来使读者理解如何搜索kd树,有点遗憾的是,[1]中1)画圆选取的中心点有误,2)向左走还是向右走有出入,3)书本中"区域与圆相交"的概念到博客里写成了"圆与切割面",4)而且也不是搜索到根结点就结束的,因该是根据需要,根结点的另外一侧可能要继续搜索等.因而还是要理解书本中例3.3的.

## 参考资料

[1] [一文搞懂k近邻（k-NN）算法（一）](https://zhuanlan.zhihu.com/p/25994179)

